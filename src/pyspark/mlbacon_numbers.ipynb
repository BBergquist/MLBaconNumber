{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from pyspark.sql import Row, SparkSession, Window, \\\n",
    "    functions as f, types as t\n",
    "from graphframes import GraphFrame\n",
    "import shutil\n",
    "import zstandard as zstd\n",
    "\n",
    "MAX_BACON_NUMBER = 9  # Previously calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparksession():\n",
    "    return (SparkSession.builder.master(\"local[*]\")\n",
    "                                .appName(\"MLBaconNumber\")\n",
    "                                .config(\"spark.jars\", \"/usr/lib/MLBaconNumber/graphframes-0.8.1-spark3.0-s_2.12.jar\")\n",
    "                                .getOrCreate())\n",
    "\n",
    "\n",
    "def get_roster(spark, compressed_roster_path):\n",
    "    decompressed_roster_path = \"/tmp/decompress_roster.csv\"\n",
    "    decompress_file(compressed_roster_path, decompressed_roster_path)\n",
    "    return read_roster_csv(spark, decompressed_roster_path)\n",
    "\n",
    "\n",
    "def get_teams(spark, compressed_teams_path):\n",
    "    decompressed_teams_path = \"/tmp/decompress_teams.csv\"\n",
    "    decompress_file(Path(compressed_teams_path).resolve(), decompressed_teams_path)\n",
    "    return read_teams_csv(spark, decompressed_teams_path)\n",
    "\n",
    "\n",
    "def decompress_file(compressed_path, decompressed_path):\n",
    "    if not exists(decompressed_path):\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "        with open(compressed_path, \"rb\") as filein, \\\n",
    "                open(decompressed_path, \"wb\") as fileout:\n",
    "            decompression_result = decompressor.copy_stream(\n",
    "                filein, fileout)\n",
    "            print(\"Size (compressed,uncompressed): {}\".format(decompression_result))\n",
    "\n",
    "\n",
    "def read_roster_csv(spark, path):\n",
    "    schema = t.StructType().add(\"year\", t.IntegerType()) \\\n",
    "        .add(\"player_id\", t.StringType()) \\\n",
    "        .add(\"last_name\", t.StringType()) \\\n",
    "        .add(\"first_name\", t.StringType()) \\\n",
    "        .add(\"bats\", t.StringType()) \\\n",
    "        .add(\"throws\", t.StringType()) \\\n",
    "        .add(\"team_id\", t.StringType()) \\\n",
    "        .add(\"position\", t.StringType())\n",
    "    return spark.read.csv(path, schema) \\\n",
    "        .drop(\"bats\", \"throws\", \"position\") \\\n",
    "        .withColumn(\"team_id\", f.concat(\"team_id\", \"year\")) \\\n",
    "        .repartition(16)\n",
    "\n",
    "\n",
    "def read_teams_csv(spark, path):\n",
    "    schema = t.StructType() \\\n",
    "        .add(\"year_id\", t.IntegerType()) \\\n",
    "        .add(\"lg_id\", t.StringType()) \\\n",
    "        .add(\"team_id\", t.StringType()) \\\n",
    "        .add(\"franch_id\", t.StringType()) \\\n",
    "        .add(\"div_id\", t.StringType()) \\\n",
    "        .add(\"rank\", t.IntegerType()) \\\n",
    "        .add(\"g\", t.IntegerType()) \\\n",
    "        .add(\"g_home\", t.IntegerType()) \\\n",
    "        .add(\"w\", t.IntegerType()) \\\n",
    "        .add(\"l\", t.IntegerType()) \\\n",
    "        .add(\"div_win\", t.StringType()) \\\n",
    "        .add(\"wc_win\", t.StringType()) \\\n",
    "        .add(\"lg_win\", t.StringType()) \\\n",
    "        .add(\"ws_win\", t.StringType()) \\\n",
    "        .add(\"r\", t.IntegerType()) \\\n",
    "        .add(\"ab\", t.IntegerType()) \\\n",
    "        .add(\"h\", t.IntegerType()) \\\n",
    "        .add(\"_2b\", t.IntegerType()) \\\n",
    "        .add(\"_3b\", t.IntegerType()) \\\n",
    "        .add(\"hr\", t.IntegerType()) \\\n",
    "        .add(\"bb\", t.IntegerType()) \\\n",
    "        .add(\"so\", t.IntegerType()) \\\n",
    "        .add(\"sb\", t.IntegerType()) \\\n",
    "        .add(\"cs\", t.IntegerType()) \\\n",
    "        .add(\"hbp\", t.IntegerType()) \\\n",
    "        .add(\"sf\", t.IntegerType()) \\\n",
    "        .add(\"ra\", t.IntegerType()) \\\n",
    "        .add(\"er\", t.IntegerType()) \\\n",
    "        .add(\"era\", t.DoubleType()) \\\n",
    "        .add(\"cg\", t.IntegerType()) \\\n",
    "        .add(\"sho\", t.IntegerType()) \\\n",
    "        .add(\"sv\", t.IntegerType()) \\\n",
    "        .add(\"ip_outs\", t.IntegerType()) \\\n",
    "        .add(\"h_a\", t.IntegerType()) \\\n",
    "        .add(\"hr_a\", t.IntegerType()) \\\n",
    "        .add(\"bb_a\", t.IntegerType()) \\\n",
    "        .add(\"so_a\", t.IntegerType()) \\\n",
    "        .add(\"e\", t.IntegerType()) \\\n",
    "        .add(\"dp\", t.IntegerType()) \\\n",
    "        .add(\"fp\", t.DoubleType()) \\\n",
    "        .add(\"name\", t.StringType()) \\\n",
    "        .add(\"park\", t.StringType()) \\\n",
    "        .add(\"attendance\", t.IntegerType()) \\\n",
    "        .add(\"bpf\", t.IntegerType()) \\\n",
    "        .add(\"ppf\", t.IntegerType()) \\\n",
    "        .add(\"team_id_br\", t.StringType()) \\\n",
    "        .add(\"team_id_lahman45\", t.StringType()) \\\n",
    "        .add(\"team_id_retro\", t.StringType())\n",
    "    return spark.read.csv(path, schema) \\\n",
    "        .select(f.concat(\"team_id_retro\", \"year_id\").alias(\"team_id\"),\n",
    "                \"year_id\", \"name\", \"lg_id\") \\\n",
    "        .repartition(16)\n",
    "\n",
    "\n",
    "def write_csv(df, filename):\n",
    "    temp_file = Path(\"/tmp\", filename)\n",
    "    output_file = Path(\"../../data\", filename)\n",
    "    df.repartition(1).write.csv(str(temp_file.absolute()),\n",
    "                                mode=\"overwrite\", header=True)\n",
    "    csv = list(temp_file.glob(\"**/*.csv\"))[0]\n",
    "    shutil.copyfile(csv, output_file.resolve())\n",
    "    shutil.rmtree(temp_file)\n",
    "\n",
    "\n",
    "def build_mlb_graph(spark, roster):\n",
    "    team_vertices = get_team_vertices(roster)\n",
    "    player_vertices = get_player_vertices(roster)\n",
    "    first_game_vertex = get_first_game_vertex(spark)\n",
    "\n",
    "    player_edges = roster.select(f.col(\"player_id\").alias(\n",
    "        \"src\"), f.col(\"team_id\").alias(\"dst\"))\n",
    "    team_edges = roster.select(f.col(\"team_id\").alias(\n",
    "        \"src\"), f.col(\"player_id\").alias(\"dst\"))\n",
    "    first_game_edges = get_first_game_edges(spark)\n",
    "\n",
    "    vertices = (team_vertices\n",
    "                .unionByName(player_vertices)\n",
    "                .unionByName(first_game_vertex))\n",
    "    edges = team_edges.unionByName(player_edges).unionByName(first_game_edges)\n",
    "    return GraphFrame(vertices, edges)\n",
    "\n",
    "\n",
    "def calculate_first_game_shortest_paths(mlb_graph):\n",
    "    return mlb_graph.shortestPaths(landmarks=[\"first_game\"])\n",
    "\n",
    "\n",
    "# Create a new graph (a tree) where the only edges are the shortest\n",
    "# paths between players to the first game\n",
    "def build_first_game_tree(spark, roster, roster_with_shortest_paths):\n",
    "    teammate_edges = get_shortest_path_edges(roster_with_shortest_paths)\n",
    "    \n",
    "    # There's some weird Spark bug where we get different results from teammate_edges\n",
    "    # Before and after the union. By saving it to storage, we ensure we get the right\n",
    "    # results.\n",
    "    temp_file = str(Path(\"/tmp\", \"teammate_edges\").absolute())\n",
    "    teammate_edges.write.parquet(temp_file, mode=\"overwrite\")\n",
    "    edges = (spark.read.parquet(temp_file)\n",
    "             .unionByName(get_first_game_edges(spark)\n",
    "                       .withColumn(\"team_id\", f.lit(\"PLAYED_IN\"))))\n",
    "\n",
    "    vertices = (get_player_vertices(roster)\n",
    "                .unionByName(get_first_game_vertex(spark)))\n",
    "    return GraphFrame(vertices, edges)\n",
    "\n",
    "\n",
    "def calculate_bacon_numbers(roster, shortest_path_tree):\n",
    "    return (shortest_path_tree.shortestPaths(landmarks=[\"first_game\"])\n",
    "            .select(f.col(\"id\").alias(\"player_id\"),\n",
    "                    (f.col(\"distances\").getItem(\"first_game\") - 1).alias(\"bacon_number\"))\n",
    "            .drop(\"distances\")\n",
    "            .join(roster, on=\"player_id\", how=\"inner\")\n",
    "            .groupBy(\"player_id\", \"last_name\", \"first_name\", \"bacon_number\")\n",
    "            .agg(f.max(\"year\").alias(\"max_year\")))\n",
    "\n",
    "\n",
    "def add_bacon_path(bacon_numbers, shortest_path_tree):\n",
    "    df = bacon_numbers.withColumn(\"v0\", f.col(\"player_id\"))\n",
    "    _ = shortest_path_tree.edges.cache().count()\n",
    "    for i in range(MAX_BACON_NUMBER + 1):\n",
    "        df = df.join(shortest_path_tree.edges,\n",
    "                     on=(f.col(\"v\" + str(i)) == f.col(\"src\")), how=\"leftouter\") \\\n",
    "            .withColumn(\"e\" + str(i+1), f.col(\"team_id\")) \\\n",
    "            .withColumn(\"v\" + str(i+1), f.col(\"dst\")) \\\n",
    "            .drop(\"src\", \"dst\", \"team_id\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_team_vertices(roster):\n",
    "    return roster.select(f.col(\"team_id\").alias(\"id\"),\n",
    "                         f.col(\"year\").alias(\"max_year\"),\n",
    "                         f.lit(\"\").alias(\"last_name\"),\n",
    "                         f.lit(\"\").alias(\"first_name\")).distinct()\n",
    "\n",
    "\n",
    "def get_player_vertices(roster):\n",
    "    return roster.groupBy(f.col(\"player_id\").alias(\"id\"),\n",
    "                          f.col(\"last_name\"), f.col(\"first_name\")) \\\n",
    "        .agg(f.max(\"year\").alias(\"max_year\"))\n",
    "\n",
    "\n",
    "def get_first_game_vertex(spark):\n",
    "    return spark.createDataFrame([\n",
    "        Row(id=\"first_game\", max_year=1871, last_name=\"\", first_name=\"\")\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_first_game_edges(spark):\n",
    "    return spark.createDataFrame([\n",
    "        (\"whitd102\", \"first_game\"),\n",
    "        (\"kimbg101\", \"first_game\"),\n",
    "        (\"paboc101\", \"first_game\"),\n",
    "        (\"allia101\", \"first_game\"),\n",
    "        (\"white104\", \"first_game\"),\n",
    "        (\"prata101\", \"first_game\"),\n",
    "        (\"sutte101\", \"first_game\"),\n",
    "        (\"carlj102\", \"first_game\"),\n",
    "        (\"bassj101\", \"first_game\"),\n",
    "        (\"selmf101\", \"first_game\"),\n",
    "        (\"mathb101\", \"first_game\"),\n",
    "        (\"foraj101\", \"first_game\"),\n",
    "        (\"goldw101\", \"first_game\"),\n",
    "        (\"lennb101\", \"first_game\"),\n",
    "        (\"caret101\", \"first_game\"),\n",
    "        (\"mince101\", \"first_game\"),\n",
    "        (\"mcdej101\", \"first_game\"),\n",
    "        (\"kellb105\", \"first_game\"),\n",
    "    ], [\"src\", \"dst\"])\n",
    "\n",
    "\n",
    "def add_shortest_paths_to_roster(roster, shortest_paths):\n",
    "    shortest_paths = shortest_paths.select(\"id\", \"distances\")\n",
    "    return (roster.join(shortest_paths, on=shortest_paths.id == roster.team_id,\n",
    "                        how=\"leftouter\")\n",
    "            .withColumn(\"team_shortest_path\", f.col(\"distances\").getItem(\"first_game\"))\n",
    "            .drop(\"id\", \"distances\")\n",
    "            .join(shortest_paths, on=shortest_paths.id == roster.player_id, how=\"leftouter\")\n",
    "            .withColumn(\"player_shortest_path\", f.col(\"distances\").getItem(\"first_game\"))\n",
    "            .drop(\"id\", \"distances\"))\n",
    "\n",
    "\n",
    "def get_shortest_path_edges(roster_shortest_paths):\n",
    "    left = roster_shortest_paths.alias(\"left\")\n",
    "    right = roster_shortest_paths.alias(\"right\")\n",
    "    return (left.join(right, on=\"team_id\", how=\"inner\")\n",
    "            .where(f.col(\"left.player_id\") != f.col(\"right.player_id\"))\n",
    "            .where(f.col(\"left.player_shortest_path\")\n",
    "                   > f.col(\"right.player_shortest_path\"))\n",
    "            .groupBy(f.col(\"left.player_id\").alias(\"src\"))\n",
    "            .agg(f.first(\"right.player_id\").alias(\"dst\"),\n",
    "                 f.min(\"team_id\").alias(\"team_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 03:46:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "compressed_roster = \"../../data/roster.csv.zst\"\n",
    "compressed_teams = \"../../data/teams.csv.zst\"\n",
    "shortest_path_filename = \"first_game_distances.csv\"\n",
    "bacon_numbers_filename = \"bacon_numbers.csv\"\n",
    "teams_filename = \"teams.csv\"\n",
    "\n",
    "spark = create_sparksession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (compressed,uncompressed): (887421, 4019466)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "roster = get_roster(spark, compressed_roster)\n",
    "\n",
    "mlb_graph = build_mlb_graph(spark, roster)\n",
    "first_game_paths = calculate_first_game_shortest_paths(mlb_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "roster_with_shortest_paths = add_shortest_paths_to_roster(\n",
    "    roster, first_game_paths)\n",
    "shortest_path_tree = build_first_game_tree(\n",
    "    spark, roster, roster_with_shortest_paths)\n",
    "bacon_numbers = calculate_bacon_numbers(roster, shortest_path_tree)\n",
    "bacon_numbers = add_bacon_path(bacon_numbers, shortest_path_tree)\n",
    "\n",
    "write_csv(bacon_numbers, bacon_numbers_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (compressed,uncompressed): (194241, 584967)\n"
     ]
    }
   ],
   "source": [
    "teams = get_teams(spark, compressed_teams)\n",
    "write_csv(teams, teams_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
